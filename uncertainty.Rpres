<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
}

.reveal h3 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.small-code pre code {
  font-size: 1em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

</style>


Plus/minus what? Let's talk about uncertainty
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/22/11
autosize: true
incremental:false
width: 1400
height: 900


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
In this world nothing can be said to be certain, except death and taxes
</h1>


Welcome to everything else!
========================================================

&nbsp;

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)
```

```{r}
library(ggplot2)
library(dplyr)
library(R.matlab)
library(rethinking)
library(ggjoy)
library(forecast)
library(gridExtra)
library(ggfortify)
library(extraDistr)
```


- How many super cool iphone lookalikes will we sell next quarter?
- When should we invest in more powerful servers?
- How many boxes of super healthy energy bars should we keep in stock?
- How long does it take to run this batch job?
- How much time do you need to get me this exposition?


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Our task today: forecast men's 400m Olympic winning times
========================================================

&nbsp;

It's 2000, just before the Olympics. This is the data we have:

```{r}
data <- readMat("olympics.mat")
male400 <- data$male400[ ,1:2]
male400 <- as.data.frame(male400) %>% rename(year = V1, seconds = V2)
male400 <- male400 %>% bind_rows(
  c(year = 2012, seconds = 43.94), c(year = 2016, seconds = 43.03))

# data up till 1996
male400_1996 <- male400 %>% filter(year < 1997)

# data from 2000
male400_2000 <- male400 %>% filter(year > 1997)

bold_text_20 <- element_text(face = "bold", size = 20)
ggplot(male400_1996, aes(x = year, y = seconds)) + geom_line() + ggtitle("Men's 400m Olympic winning times 1986-1996") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)


```


&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


42.3 seconds? 42.1?
========================================================

&nbsp;

&nbsp;

Linear regression says 42.33.

Whatever we say, it's pretty likely to be wrong...


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


How do we deal with this?
========================================================

&nbsp;

&nbsp;

Let's better not commit to a point estimate... prediction intervals to the rescue!




<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Wait: prediction intervals? Wasn't that called "confidence intervals?"
========================================================
class:small-code

&nbsp;

Let's take the example of linear regression.


```{r, echo=TRUE}
(fit <- lm(seconds ~ year, male400_1996))
```

&nbsp;

Here's the point prediction:

```{r, echo=TRUE}
# this would yield the same result
# fit$coefficients[1] + fit$coefficients[2] * 2000
fit %>% predict(newdata = data.frame(year = c(2000)))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Let's try getting those (confidence? prediction?) intervals!
========================================================
class:small-code

&nbsp;

Confidence intervals:

```{r, echo=TRUE}
fit %>% predict(newdata = data.frame(year = c(2000)), interval = "confidence")
```

Prediction intervals:

```{r, echo=TRUE}
fit %>% predict(newdata = data.frame(year = c(2000)), interval = "prediction")
```


Quite a difference! So which one do we take?

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



One step back: sampling variation and standard errors
========================================================

&nbsp;

- In single-variable linear regression we estimate an _intercept_ and a _slope_

$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$\hat \beta_1 = cor(y, x) \frac{sd(y)}{sd(x)}$

- ... that together make up the equation of a line

$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

- These estimates vary depending on the _sample_ they are estimated from
- The statistical method gives us _standard errors_ for these estimates


$\sigma_{\hat \beta_0} = \hat \sigma^2 \left(\frac{1}{n} + \frac{\bar x^2}{\sum_{i=1}^n (x_i - \bar x)^2 }\right)$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$\sigma_{\hat \beta_1} = \hat \sigma^2 / \sum_{i=1}^n (x_i - \bar x)^2$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
with &nbsp; 
$\hat \sigma^2 =\frac{1}{n-2}\sum_{i=1}^n e_i^2$

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



From standard errors, we can construct confidence intervals
========================================================
class:small-code

&nbsp;

- for the parameters
- for the line overall

Let's do this manually for the parameters.
A 95% confidence interval for the intercept would look like this:

```{r, echo=TRUE}
intercept_est <- summary(fit)$coefficients[1,1]
intercept_se <- summary(fit)$coefficients[1,2]
(conf_interval <- intercept_est + c(-1, 1) * qt(.975, df = fit$df) * intercept_se)
```

&nbsp;

Same procedure for the slope:

```{r, echo=TRUE}
slope_est <- summary(fit)$coefficients[2,1]
slope_se <- summary(fit)$coefficients[2,2]
(conf_interval <- slope_est + c(-1, 1) * qt(.975, df = fit$df) * slope_se)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Which means we can say
========================================================

&nbsp;

&nbsp;


> 
"with 95% confidence, we estimate that having 4 years pass results in a decrease in the men's 400m Olympic winning times of 0.07 to 0.1 seconds"

&nbsp;

&nbsp;

... which reflects our uncertainty about the slope, not the points on the line.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



A confidence interval for the regression line
========================================================

&nbsp;

We need the standard error for a point $x_0$ on the regression line: &nbsp; &nbsp; &nbsp; &nbsp; $\hat \sigma\sqrt{\frac{1}{n} +  \frac{(x_0 - \bar x)^2}{\sum_{i=1}^n (x_i - \bar x)^2}}$

This reflects the amount of uncertainty due to our estimates being based on _sample variation_.
Uncertainty is smallest near the mean of the predictor ($\bar x$).

```{r, fig.width=16}
df <- male400_1996 %>% bind_cols(is_pred = factor(rep(0, nrow(male400_1996)), levels = c(0,1)))
df <-  df %>% bind_rows(data.frame(year = seq(2000, 2016, by=4), is_pred = factor(rep(1,5), levels = c(0,1))))
preds <- fit %>% predict(newdata = df, interval = "confidence")
df[25:29,2] <- preds[25:29,1]
ggplot(df, aes(x = year, y=seconds)) + geom_point(aes(color = is_pred, shape = is_pred), size = 3) + 
  geom_ribbon(aes(ymin = preds[ , 2], ymax = preds[ , 3]), alpha = 0.2) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



But... there's an additional source of uncertainty!
========================================================

&nbsp;

For sure the predictor $x$ (the year we're in) cannot be held 100% responsible for the outcome $y$ (the 400m winning time).

The standard error for an actual _prediction_ is: &nbsp; &nbsp; &nbsp; &nbsp; $\hat \sigma\sqrt{1 + \frac{1}{n} +  \frac{(x_0 - \bar x)^2}{\sum_{i=1}^n (x_i - \bar x)^2}}$


```{r, fig.width=16}
df <- male400_1996 %>% bind_cols(is_pred = factor(rep(0, nrow(male400_1996)), levels = c(0,1)))
df <-  df %>% bind_rows(data.frame(year = seq(2000, 2016, by=4), is_pred = factor(rep(1,5), levels = c(0,1))))
preds <- fit %>% predict(newdata = df, interval = "prediction")
df[25:29,2] <- preds[25:29,1]
ggplot(df, aes(x = year, y=seconds)) + geom_point(aes(color = is_pred, shape = is_pred), size = 3) + 
  geom_ribbon(aes(ymin = preds[ , 2], ymax = preds[ , 3]), alpha = 0.2) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Fine, but... that's all specific to linear regression...
========================================================

&nbsp;

&nbsp;

- What if we were using, say, ARIMA for forecasting that time series?

- What if we used some custom method that does not come complete with standard errors / confidence intervals and all?


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Let's try ARIMA on the 400m winning times!
========================================================
class:small-code


&nbsp;

```{r, echo=TRUE}
arima_fit <- auto.arima(male400_1996$seconds, allowdrift = FALSE)
arima_fit$coef
```

&nbsp;

We get standard errors for the parameter estimations:

```{r, echo=TRUE}
sqrt(diag(vcov(arima_fit)))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



ARIMA prediction intervals
========================================================
class:small-code

&nbsp;

... which means we can get prediction intervals here, too:

```{r, fig.width=16}
autoplot(forecast(arima_fit, h=5))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Fine. What if we're not automagically given those precision intervals?
========================================================

&nbsp;

&nbsp;

We'll probably need to pull ourselves up by our own bootstraps...

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



The bootstrap
========================================================

&nbsp;

- Ideally, we'd compute a parameter's standard error from many repeated samples
- Unfortunately, most of the time we just have a single sample
- Idea: create synthetical samples from the original one, using _sampling with replacement_

<figure>
    <img src='bootstrap.png' width='40%'/>
    <figcaption>Source: [1]</figcaption>
</figure>

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


So it all depends on the specifics of the method we're using...
========================================================

&nbsp;

&nbsp;

Wouldn't it be nice if there was a unified, intuitive approach?

&nbsp;

Well ... there is.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Let's do it the Bayesian way!
========================================================

&nbsp;

In Bayesian statistics:

- the _data_ is _given_, and the _parameters_ are _random_ (and thus have distributions, too!)
- as new data comes in, we update our expectations - according to famous _Bayes theorem_

$$P(A|B) = \frac{P(B|A) * P(A)}{P(B)}$$ 

- uncertainty estimates are "for free", as we look at the complete posterior distribution

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Bayes - in practice - in R
========================================================

&nbsp;

R packages for Bayesian modeling (using the <a href="mc-stan.org">_Stan_</a> backend for MCMC sampling):
- _rethinking_ (by R. McElreath, author of awesome _Statistical Rethinking_) - we'll use this one soon
- rstan (uses Stan's C-like DSL)
- rstanarm, brms (higher level interfaces in the style of R's usual model fitting syntax)


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



No Bayes without priors
========================================================

&nbsp;

- Common practice: use uniform priors (feigning total ignorance)
- But often, we have at least _some_ information!

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Choosing priors
========================================================

&nbsp;

What do we know?

- mean is somewhere around 46 => have intercept centered at 46, with high variance
- want to conservatively estimate slope: center at 0, but use high variance
- noise variance can only be >= 0: use half-cauchy centered at 0

```{r, fig.width=12, fig.height=6}

bold_text_15 <- element_text(face = "bold", size = 15)
g1 <- ggdistribution(dnorm, x = 25:65, mean = 46, sd=30, fill = 'red') + ggtitle("Prior for intercept") +
  theme(title = bold_text_15)
g2 <- ggdistribution(dnorm, x = -3:3, mean = 0, sd=10, fill = 'blue') + ggtitle("Prior for slope") +
  theme(title = bold_text_15)
g3 <- ggdistribution(dhcauchy, x = seq(-3,3,by=0.01), sigma = 10, fill = "green") + ggtitle("Prior for variance") +
  theme(title = bold_text_15)
grid.arrange(g1, g2, g3, ncol=3)
```

&nbsp;


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



The model
========================================================
class:small-code

&nbsp;


```{r, echo = TRUE, eval = FALSE}

model <- map2stan(
  alist(
    seconds ~ dnorm(mu, sigma),
    mu <- a + b*year,
    a ~ dnorm(46, 30),
    b ~ dnorm(0,10),
    sigma ~ dcauchy(0, 10)
  ),
  data = male400_1996,
  iter = 6000,
  chains = 4,
  verbose = FALSE
)

```

... sampling ... sampling ... sampling ... and here we go:


```{r, echo=TRUE, eval = FALSE}
precis(model)
```

```
        Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a     159.34  23.66     123.89     200.52    80 1.00
b      -0.06   0.01      -0.08      -0.04    80 1.00
sigma   1.61   0.41       1.05       2.12    67 1.01
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



What about the regression line?
========================================================

&nbsp;

With all the sampling that's been going on, now we don't just have 1 line, but many!
We extract synchronous parameter values from the samples and construct one line per sample:

```{r, echo = FALSE}
post <- extract.samples(model)

### plot regression lines for samples from posterior
plot( male400_1996$year , male400_1996$seconds ,
      xlim=range(male400_1996$year) , ylim=range(male400_1996$seconds) ,
      col=rangi2 , xlab="" , ylab="" )
for ( i in 1:20 )
  abline( a=post$a[i] , b=post$b[i] , col=col.alpha("black",0.3))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



##############################################################################################

TBD 

- bootstrap??
- bayes
- time series??
- dropout
- end with real values 


Sources
========================================================
class:smalltext

[] 

