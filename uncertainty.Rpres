<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
}

.reveal h3 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.small-code pre code {
  font-size: 1em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

</style>


Plus/minus what? Let's talk about uncertainty
========================================================
author: Sigrid Keydana, Trivadis
date: 2017/22/11
autosize: true
incremental:false
width: 1400
height: 900


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
In this world nothing can be said to be certain, except death and taxes
</h1>


Welcome to everything else!
========================================================

&nbsp;

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)
```

```{r}
library(ggplot2)
library(tidyr)
library(dplyr)
library(R.matlab)
library(rethinking)
library(ggjoy)
library(forecast)
library(gridExtra)
library(ggfortify)
library(extraDistr)
library(stringr)
```


- How many super cool iphone lookalikes will we sell next quarter?
- When should we invest in more powerful servers?
- How many boxes of super healthy energy bars should we keep in stock?
- How long does it take to run this batch job?
- How much time do you need to get me this exposition?


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
What we do: sports forecasts
</h1>


Our task today: forecast men's 400m Olympic winning times
========================================================

&nbsp;

It's 2000, just before the Olympics. This is the data we have:

```{r}
data <- readMat("olympics.mat")
male400 <- data$male400[ ,1:2]
male400 <- as.data.frame(male400) %>% rename(year = V1, seconds = V2)
male400 <- male400 %>% bind_rows(
  c(year = 2012, seconds = 43.94), c(year = 2016, seconds = 43.03))

# data up till 1996
male400_1996 <- male400 %>% filter(year < 1997)

# data from 2000
male400_2000 <- male400 %>% filter(year > 1997)

bold_text_20 <- element_text(face = "bold", size = 20)
ggplot(male400_1996, aes(x = year, y = seconds)) + geom_line() + ggtitle("Men's 400m Olympic winning times 1986-1996") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20)


```


&nbsp;

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


42.3 seconds? 42.1?
========================================================

&nbsp;

&nbsp;

Linear regression says 42.33.

Whatever we say, it's pretty likely to be wrong...


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


How do we deal with this?
========================================================

&nbsp;

&nbsp;

Let's better not commit to a point estimate... prediction intervals to the rescue!




<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Prediction intervals - linear regression
</h1>


Wait: prediction intervals? Wasn't that called "confidence intervals?"
========================================================
class:small-code

&nbsp;

Let's take the example of linear regression.


```{r, echo=TRUE}
(fit <- lm(seconds ~ year, male400_1996))
```

&nbsp;

Here's the point prediction:

```{r, echo=TRUE}
# this would yield the same result
# fit$coefficients[1] + fit$coefficients[2] * 2000
fit %>% predict(newdata = data.frame(year = c(2000)))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Let's try getting those (confidence? prediction?) intervals!
========================================================
class:small-code

&nbsp;

Confidence intervals:

```{r, echo=TRUE}
fit %>% predict(newdata = data.frame(year = c(2000)), interval = "confidence")
```

Prediction intervals:

```{r, echo=TRUE}
fit %>% predict(newdata = data.frame(year = c(2000)), interval = "prediction")
```


Quite a difference! So which one do we take?

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



One step back: sampling variation and standard errors
========================================================

&nbsp;

- In single-variable linear regression we estimate an _intercept_ and a _slope_

$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$\hat \beta_1 = cor(y, x) \frac{sd(y)}{sd(x)}$

- ... that together make up the equation of a line

$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

- These estimates vary depending on the _sample_ they are estimated from
- The statistical method gives us _standard errors_ for these estimates


$\sigma_{\hat \beta_0} = \hat \sigma^2 \left(\frac{1}{n} + \frac{\bar x^2}{\sum_{i=1}^n (x_i - \bar x)^2 }\right)$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
$\sigma_{\hat \beta_1} = \hat \sigma^2 / \sum_{i=1}^n (x_i - \bar x)^2$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
with &nbsp; 
$\hat \sigma^2 =\frac{1}{n-2}\sum_{i=1}^n e_i^2$

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



From standard errors, we can construct confidence intervals
========================================================
class:small-code

&nbsp;

- for the parameters
- for the line overall

Let's do this manually for the parameters.
A 95% confidence interval for the intercept would look like this:

```{r, echo=TRUE}
intercept_est <- summary(fit)$coefficients[1,1]
intercept_se <- summary(fit)$coefficients[1,2]
(conf_interval <- intercept_est + c(-1, 1) * qt(.975, df = fit$df) * intercept_se)
```

&nbsp;

Same procedure for the slope:

```{r, echo=TRUE}
slope_est <- summary(fit)$coefficients[2,1]
slope_se <- summary(fit)$coefficients[2,2]
(conf_interval <- slope_est + c(-1, 1) * qt(.975, df = fit$df) * slope_se)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Which means we can say
========================================================

&nbsp;

&nbsp;


> 
"with 95% confidence, we estimate that having 4 years pass results in a decrease in the men's 400m Olympic winning times of 0.07 to 0.1 seconds"

&nbsp;

&nbsp;

... which reflects our uncertainty about the slope, not the points on the line.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



A confidence interval for the regression line
========================================================

&nbsp;

We need the standard error for a point $x_0$ on the regression line: &nbsp; &nbsp; &nbsp; &nbsp; $\hat \sigma\sqrt{\frac{1}{n} +  \frac{(x_0 - \bar x)^2}{\sum_{i=1}^n (x_i - \bar x)^2}}$

This reflects the amount of uncertainty due to our estimates being based on _sample variation_.
Uncertainty is smallest near the mean of the predictor ($\bar x$).

```{r, fig.width=16}
df <- male400_1996 %>% bind_cols(is_pred = factor(rep(0, nrow(male400_1996)), levels = c(0,1)))
df <-  df %>% bind_rows(data.frame(year = seq(2000, 2016, by=4), is_pred = factor(rep(1,5), levels = c(0,1))))
preds <- fit %>% predict(newdata = df, interval = "confidence")
df[25:29,2] <- preds[25:29,1]
ggplot(df, aes(x = year, y=seconds)) + geom_point(aes(color = is_pred, shape = is_pred), size = 3) + 
  geom_ribbon(aes(ymin = preds[ , 2], ymax = preds[ , 3]), alpha = 0.2) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016", subtitle = "Confidence intervals from least squares") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



But... there's an additional source of uncertainty!
========================================================

&nbsp;

For sure the predictor $x$ (the year we're in) cannot be held 100% responsible for the outcome $y$ (the 400m winning time).

The standard error for an actual _prediction_ is: &nbsp; &nbsp; &nbsp; &nbsp; $\hat \sigma\sqrt{1 + \frac{1}{n} +  \frac{(x_0 - \bar x)^2}{\sum_{i=1}^n (x_i - \bar x)^2}}$


```{r, fig.width=16}
df <- male400_1996 %>% bind_cols(is_pred = factor(rep(0, nrow(male400_1996)), levels = c(0,1)))
df <-  df %>% bind_rows(data.frame(year = seq(2000, 2016, by=4), is_pred = factor(rep(1,5), levels = c(0,1))))
preds <- fit %>% predict(newdata = df, interval = "prediction")
df[25:29,2] <- preds[25:29,1]
ggplot(df, aes(x = year, y=seconds)) + geom_point(aes(color = is_pred, shape = is_pred), size = 3) + 
  geom_ribbon(aes(ymin = preds[ , 2], ymax = preds[ , 3]), alpha = 0.2) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016", subtitle = "Prediction intervals from least squares") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")
```



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Fine, but... that's all specific to linear regression...
========================================================

&nbsp;

&nbsp;

- What if we were using, say, ARIMA for forecasting that time series?

- What if we used some custom method that does not come complete with standard errors / confidence intervals and all?


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Prediction intervals - ARIMA
</h1>


Let's try ARIMA on the 400m winning times!
========================================================
class:small-code


&nbsp;

```{r, echo=TRUE}
arima_fit <- auto.arima(male400_1996$seconds, allowdrift = FALSE)
arima_fit$coef
```

&nbsp;

We get standard errors for the parameter estimates:

```{r, echo=TRUE}
sqrt(diag(vcov(arima_fit)))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



ARIMA prediction intervals
========================================================
class:small-code

&nbsp;

... which means we can get prediction intervals here, too:

```{r, fig.width=16}
autoplot(forecast(arima_fit, h=5))
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Fine. What if we're not automagically given those prediction intervals?
========================================================

&nbsp;

&nbsp;

We'll probably need to pull ourselves up by our own bootstraps...

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Generic solution: the bootstrap
</h1>


The bootstrap
========================================================

&nbsp;

- Ideally, we'd compute a parameter's standard error from many repeated samples
- Unfortunately, most of the time we just have a single sample
- Idea: create synthetical samples from the original one, using _sampling with replacement_

<figure>
    <img src='bootstrap.png' width='40%'/>
    <figcaption>Source: [1]</figcaption>
</figure>

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


So it all depends on the specifics of the method we're using...
========================================================

&nbsp;

&nbsp;

Wouldn't it be nice if there was a unified, intuitive approach?

&nbsp;

Well ... there is.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Uncertainty for free: The Bayesian approach
</h1>


Let's do it the Bayesian way!
========================================================

&nbsp;

In Bayesian statistics:

- the _data_ is _given_, and the _parameters_ are _random_ (and thus have distributions, too!)
- as new data comes in, we update our expectations - according to famous _Bayes theorem_

$$P(A|B) = \frac{P(B|A) * P(A)}{P(B)}$$ 

- uncertainty estimates are "for free", as we look at the complete posterior distribution

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />




Bayes - in practice - using R
========================================================

&nbsp;

R packages for Bayesian modeling (using the <a href="mc-stan.org">_Stan_</a> backend for MCMC sampling):
- _rethinking_ (by R. McElreath, author of awesome _Statistical Rethinking_) - we'll use this one soon
- rstan (uses Stan's C-like DSL)
- rstanarm, brms (higher level interfaces in the style of R's usual model fitting syntax)


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



No Bayes without priors...
========================================================

&nbsp;

- Common practice: use uniform priors (feigning total ignorance)
- But often, we have at least _some_ information!

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Choosing priors for the men's 400m
========================================================

&nbsp;

What do we know?

- mean is somewhere around 46 => have intercept centered at 46, with high variance
- want to conservatively estimate slope: center at 0, but use high variance
- noise variance can only be >= 0: use half-cauchy centered at 0

&nbsp;

```{r, fig.width=12, fig.height=6}

bold_text_15 <- element_text(face = "bold", size = 15)
g1 <- ggdistribution(dnorm, x = 25:65, mean = 46, sd=30, fill = 'red') + ggtitle("Prior for intercept") +
  theme(title = bold_text_15)
g2 <- ggdistribution(dnorm, x = -3:3, mean = 0, sd=10, fill = 'blue') + ggtitle("Prior for slope") +
  theme(title = bold_text_15)
g3 <- ggdistribution(dhcauchy, x = seq(-3,3,by=0.01), sigma = 10, fill = "green") + ggtitle("Prior for variance") +
  theme(title = bold_text_15)
grid.arrange(g1, g2, g3, ncol=3)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Model specification
========================================================

&nbsp;


```{r, echo = TRUE, results = "hide"}
require(rethinking)

model <- map2stan(
  alist(
    seconds ~ dnorm(mu, sigma),
    mu <- a + b*year,
    a ~ dnorm(46, 30),
    b ~ dnorm(0,10),
    sigma ~ dcauchy(0, 10)
  ),
  data = male400_1996,
  iter = 6000,
  chains = 4,
  verbose = FALSE
)

```



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />





Model results
========================================================

&nbsp;


```{r, echo=TRUE}
precis(model)
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



So, what about the regression line?
========================================================

&nbsp;

With all the sampling that's been going on, now we don't just have 1 line, but many!

We extract parameter values from the samples and construct one line per sample:

```{r, echo = FALSE}
post <- extract.samples(model)

### plot regression lines for samples from posterior
plot( male400_1996$year , male400_1996$seconds ,
      xlim=range(male400_1996$year) , ylim=range(male400_1996$seconds) ,
      col=rangi2 , xlab="" , ylab="" )
for ( i in 1:100 )
  abline( a=post$a[i] , b=post$b[i] , col=col.alpha("black",0.2))
```


We can immediately _see_ the uncertainty in our model!

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Posterior intervals for the regression line
========================================================
class:smalltext

&nbsp;

In the Bayesian framework, the equivalent of a confidence interval is a _credible interval_.
Equivalent? Well... except
- credible intervals really have an intuitive interpretation
- credible intervals really contain the highest density values

```{r, results= "hide", fig.width=16}
df <- male400_1996 %>% bind_cols(is_pred = factor(rep(0, nrow(male400_1996)), levels = c(0,1)))
df <-  df %>% bind_rows(data.frame(year = seq(2000, 2016, by=4), is_pred = factor(rep(1,5), levels = c(0,1))))
mu <- link(model, data=df)
mu_mean <- apply(mu, 2, mean)
mu_HPDI <- apply(mu, 2, HPDI , prob=0.95) %>% t()
df[25:29,2] <- mu_mean[25:29]
```

```{r}
ggplot(df, aes(x = year, y=seconds)) + geom_point(aes(color = is_pred, shape = is_pred), size = 3) + 
  geom_ribbon(aes(ymin = mu_HPDI[ , 1], ymax = mu_HPDI[ , 2]), alpha = 0.2) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016", subtitle = "Credible intervals from Bayesian linear regression") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")

```

Credible intervals are easily constructed using existing samples from the parameters' posterior.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Prediction intervals
========================================================
class:smalltext

&nbsp;

Of course, here again we're not really interested in predicting averages, but individual examples.

We want _prediction intervals_:

```{r, results= "hide", fig.width=16}
sims <- sim(model, data=df)
sim_PI <- apply(sims, 2, PI, prob=0.95) %>% t()
```

```{r}
ggplot(df, aes(x = year, y=seconds)) + geom_point(aes(color = is_pred, shape = is_pred), size = 3) + 
  geom_ribbon(aes(ymin = sim_PI[ , 1], ymax = sim_PI[ , 2]), alpha = 0.2) +
  ggtitle("Men's 400m Olympic winning times 1986-1996 and predictions for 2000-2016", subtitle = "Prediction intervals from Bayesian linear regression") +
  xlab("year") + ylab("seconds") + 
  theme(title = bold_text_20, axis.title = bold_text_20, axis.text = bold_text_20, legend.position = "none")

```

Prediction intervals are computed from the complete _posterior predictive_ distribution.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



So that's uncertainty intervals...
========================================================

&nbsp;

... didn't you say we'd look at _complete distributions_ for our predictions?

Yes. That's the _posterior predictive_ distribution:

$$p(\tilde{y}|X, y, \tilde{x}, \theta) = \int p(\tilde{y}|\tilde{x},\theta) \, p(\theta|X,y) \operatorname{d}\!\theta$$

The posterior predictive is a weighted average of predictions, computed over all possible parameter values.

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Posterior predictive - the whole picture
========================================================

&nbsp;

We actually have a distribution of predictions for every point in time.

```{r, fig.width=16}
df <- data.frame(sims)
df <- df %>% 
  gather(key = "year", value = "secs") %>% 
  mutate(year = as.numeric(str_sub(year,2,3)))

convert_years <- function(x) 1896 + 4* (x-1)

ggplot(df, aes(x = secs, y = year, group = year, height = ..density..)) + geom_joy(stat = "density", rel_min_height = 0.01, scale=4) +
  scale_x_reverse() + scale_y_reverse() + theme_joy(grid = FALSE) +
  scale_y_continuous(labels = convert_years) 
  
```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Fine. So Bayes solves it all...
========================================================

&nbsp;

&nbsp;

... what if I'm using deep learning?

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Uncertainty in deep neural networks
</h1>



Uncertainty in deep learning
========================================================

&nbsp;

- Normally, in neural networks, the outputs are point predictions (possibly in the form of class probabilities)
- Looking at NN architecture, it is not self-evident how to extract confidence/prediction intervals from a network
- We could always apply the bootstrap approach, or even simple ensembling, to at least get a feel for the variability of the net's estimates
- Several other approaches have been suggested, among them the use of _dropout_ to calculate uncertainty:

<figure>
    <img src='dropout.png' width='50%'/>
    <figcaption>Source: [1]</figcaption>
</figure>


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Dropout networks as variational inference in Gaussian processes
========================================================

&nbsp;

"... We'll see that what we did above, averaging forward passes through the network, is equivalent to Monte Carlo integration over a Gaussian process posterior approximation."
Yarin Gal, <a href=""></a>

```{r}
library(keras)
K <- keras::backend()

n_samples <- 1000
n_features <- 1
n_hidden1 <- 128
n_hidden2 <- 128
n_output <- 1

learning_rate <- 1e-6
num_epochs <- 100
batch_size <- n_samples / 100

dropout <- 0.5
l2 <- 0.1
X_train <- matrix(c(-500, -200, 1:996 + rnorm(996, mean = 0, sd = 10), 1200, 1500),
                  nrow = 1000, ncol = 1)
dim(X_train)
coefs <- c(0.5)
mu <- X_train %*% coefs
sigma = 2
y_train <- rnorm(n_samples, mu, sigma)

fit <- lm(y_train ~ X_train)
summary(fit)
 
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = n_hidden1, activation = 'relu', input_shape = c(n_features)) %>% 
  layer_dropout(rate = dropout) %>% 
  layer_activity_regularization(l1=0, l2=l2) %>%
  layer_dense(units = n_hidden2, activation = 'relu') %>%
  layer_dropout(rate = dropout) %>%
  layer_activity_regularization(l1=0, l2=l2) %>%
  layer_dense(units = n_output, activation = 'linear')

model %>% summary()

model %>% compile(
  loss = 'mean_squared_error',
  optimizer = optimizer_adam())
  
history <- model %>% fit(
    X_train, y_train, 
    epochs = num_epochs, batch_size = batch_size, 
    validation_split = 0.2
)

model$layers
get_output = K$`function`(list(model$layers[[1]]$input, K$learning_phase()), list(model$layers[[7]]$output))

n <- 20
inclusion_prob <- 1-dropout
num_samples <- nrow(X_train)
weight_decay <- l2
length_scale <- 0.5

preds <- matrix(NA, nrow = nrow(X_train), ncol = n)
dim(preds)
for(i in seq_len(n)) {
  # train mode = 1
  preds[ ,i] <- get_output(list(X_train, 1))[[1]]
}
dim(preds)

predictive_mean <- apply(preds, 1, mean)
predictive_var <-apply(preds, 1, var)
tau <- length_scale^2 * inclusion_prob / (2 * num_samples * weight_decay)
predictive_var <- predictive_var + tau^-1

df <- data.frame(
  x = as.vector(X_train),
  pred_mean = predictive_mean,
  lwr = predictive_mean - sqrt(predictive_var),
  upr = predictive_mean + sqrt(predictive_var)
)

ggplot(df, aes(x = x, y=predictive_mean)) + geom_point() + 
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2) 


```


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Posterior predictive - the whole picture
========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Posterior predictive - the whole picture
========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


Posterior predictive - the whole picture
========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />



Posterior predictive - the whole picture
========================================================

&nbsp;



<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />





  

Sources
========================================================
class:smalltext

[1] http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf


